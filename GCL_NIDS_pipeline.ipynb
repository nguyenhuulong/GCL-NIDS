{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408c20e5",
   "metadata": {},
   "source": [
    "# GCL-NIDS — Graph Contrastive Learning for NIDS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c5aa0",
   "metadata": {},
   "source": [
    "### Extension: Contrastive Pretraining\n",
    "Notebook này không chỉ huấn luyện GNN theo cách supervised, mà còn bổ sung một nhánh **Graph Contrastive Learning (GCL)**:\n",
    "1. Pretrain encoder bằng contrastive loss (InfoNCE) trên đồ thị augmented.\n",
    "2. Fine-tune classifier trên tập gán nhãn nhỏ.\n",
    "3. So sánh với baseline supervised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675330e",
   "metadata": {},
   "source": [
    "## 1. Import & setup GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974e0988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"d:\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"d:\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"d:\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"d:\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_16300\\3553889354.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"d:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "d:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e026016",
   "metadata": {},
   "source": [
    "## 2. Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2628dc",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "**Datasets used in this notebook:**\n",
    "- BoT-IoT (5% subset) — `./processed/bot/`\n",
    "- CIC-IDS2018 (9 selected days) — `./processed/cic18/`\n",
    "- UNSW — `./processed/unsw/`\n",
    "\n",
    "**Notes for reproducibility**\n",
    "1. Download data (link in README).\n",
    "2. This notebook reads processed `.npy` files. If your dataset is too big, set `N_sample` or run on a subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f50ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trước khi gom lớp: (array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), array([37000,  3496,   583,  4089, 11132,   677,  6062,    44,   378,\n",
      "       18871]))\n",
      "Sau khi gom lớp: (array([0, 1]), array([37000, 45332]))\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu UNSW\n",
    "X_train = np.load(\"./processed/unsw/X_train_unsw.npy\")\n",
    "X_test = np.load(\"./processed/unsw/X_test_unsw.npy\")\n",
    "y_train = np.load(\"./processed/unsw/y_train_unsw.npy\")\n",
    "y_test = np.load(\"./processed/unsw/y_test_unsw.npy\")\n",
    "\n",
    "print(\"Trước khi gom lớp:\", np.unique(y_train, return_counts=True))\n",
    "\n",
    "# Gom về binary: 0 = normal, 1 = attack\n",
    "y_train = (y_train != 0).astype(int)\n",
    "y_test = (y_test != 0).astype(int)\n",
    "\n",
    "print(\"Sau khi gom lớp:\", np.unique(y_train, return_counts=True))\n",
    "\n",
    "# Chuẩn hóa feature\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80348f22",
   "metadata": {},
   "source": [
    "## 3. Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee33cf",
   "metadata": {},
   "source": [
    "Design choices and knobs (explained):\n",
    "- **Node definition:** each node = a network flow (Netflow record). Alternative: host-level nodes (endpoint-centric).\n",
    "- **Edge definition:** KNN on feature vectors (approximate/FAISS recommended) or explicit IP→IP edges (flow source→dest). Current notebook: **KNN with k=5** to capture local similarity in feature space.\n",
    "- **Sampling options:** `N_sample` (for speed), `balance` (stratified sampling to avoid class collapse).\n",
    "- **Symmetrize edges:** we add both (i→j) and (j→i) for undirected message passing.\n",
    "- **Performance tradeoffs:** building full KNN on millions of points is expensive — use `faiss-cpu` or sampling.\n",
    "\n",
    "**What to report in experiments:** nodes, edges, avg degree, build time, peak RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea84245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build_graph] Using full dataset: 82332 samples\n",
      "[build_graph] Fitting NearestNeighbors (n_neighbors=6)...\n",
      "  processed 5000/82332 (elapsed 23.2s)\n",
      "  processed 10000/82332 (elapsed 23.2s)\n",
      "  processed 15000/82332 (elapsed 23.3s)\n",
      "  processed 20000/82332 (elapsed 23.4s)\n",
      "  processed 25000/82332 (elapsed 23.5s)\n",
      "  processed 30000/82332 (elapsed 23.5s)\n",
      "  processed 35000/82332 (elapsed 23.5s)\n",
      "  processed 40000/82332 (elapsed 23.6s)\n",
      "  processed 45000/82332 (elapsed 23.6s)\n",
      "  processed 50000/82332 (elapsed 23.6s)\n",
      "  processed 55000/82332 (elapsed 23.6s)\n",
      "  processed 60000/82332 (elapsed 23.8s)\n",
      "  processed 65000/82332 (elapsed 23.8s)\n",
      "  processed 70000/82332 (elapsed 23.8s)\n",
      "  processed 75000/82332 (elapsed 23.8s)\n",
      "  processed 80000/82332 (elapsed 23.8s)\n",
      "[build_graph] Done! nodes=82332, edges=823320, time=25.8s\n",
      "[build_graph] Sampled 100000/175341 samples\n",
      "[build_graph] Fitting NearestNeighbors (n_neighbors=6)...\n",
      "  processed 5000/100000 (elapsed 33.2s)\n",
      "  processed 10000/100000 (elapsed 33.2s)\n",
      "  processed 15000/100000 (elapsed 33.3s)\n",
      "  processed 20000/100000 (elapsed 33.4s)\n",
      "  processed 25000/100000 (elapsed 33.4s)\n",
      "  processed 30000/100000 (elapsed 33.5s)\n",
      "  processed 35000/100000 (elapsed 33.5s)\n",
      "  processed 40000/100000 (elapsed 33.5s)\n",
      "  processed 45000/100000 (elapsed 33.6s)\n",
      "  processed 50000/100000 (elapsed 33.6s)\n",
      "  processed 55000/100000 (elapsed 33.7s)\n",
      "  processed 60000/100000 (elapsed 33.7s)\n",
      "  processed 65000/100000 (elapsed 33.8s)\n",
      "  processed 70000/100000 (elapsed 33.8s)\n",
      "  processed 75000/100000 (elapsed 33.8s)\n",
      "  processed 80000/100000 (elapsed 33.8s)\n",
      "  processed 85000/100000 (elapsed 33.9s)\n",
      "  processed 90000/100000 (elapsed 33.9s)\n",
      "  processed 95000/100000 (elapsed 34.0s)\n",
      "[build_graph] Done! nodes=100000, edges=1000000, time=36.4s\n"
     ]
    }
   ],
   "source": [
    "def build_graph(X, y=None, k=5, log_every=5000, N_sample=100000, random_state=42):\n",
    "    \"\"\"\n",
    "    X: numpy array (N, d)\n",
    "    y: label (N,)\n",
    "    k: số láng giềng\n",
    "    N_sample: số mẫu tối đa để xây đồ thị (default=100k)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from torch_geometric.data import Data\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    N = X.shape[0]\n",
    "    if N > N_sample:\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        idx = rng.choice(N, size=N_sample, replace=False)\n",
    "        X = X[idx]\n",
    "        y = y[idx] if y is not None else None\n",
    "        print(f\"[build_graph] Sampled {N_sample}/{N} samples\")\n",
    "    else:\n",
    "        print(f\"[build_graph] Using full dataset: {N} samples\")\n",
    "\n",
    "    print(f\"[build_graph] Fitting NearestNeighbors (n_neighbors={k+1})...\")\n",
    "    nbrs = NearestNeighbors(\n",
    "        n_neighbors=k+1, algorithm='auto', n_jobs=-1).fit(X)\n",
    "    _, neighbors = nbrs.kneighbors(X)\n",
    "\n",
    "    edge_index = []\n",
    "    for i in range(X.shape[0]):\n",
    "        if i % log_every == 0 and i > 0:\n",
    "            print(\n",
    "                f\"  processed {i}/{X.shape[0]} (elapsed {time.time()-start:.1f}s)\")\n",
    "        for j in neighbors[i][1:]:\n",
    "            edge_index.append([i, j])\n",
    "            edge_index.append([j, i])  # symmetrize\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = Data(\n",
    "        x=torch.tensor(X, dtype=torch.float),\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor(y, dtype=torch.long) if y is not None else None\n",
    "    )\n",
    "    print(\n",
    "        f\"[build_graph] Done! nodes={data.num_nodes}, edges={data.num_edges}, time={time.time()-start:.1f}s\")\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = build_graph(X_train, y_train, k=5).to(device)\n",
    "test_data = build_graph(X_test,  y_test,  k=5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290dca37",
   "metadata": {},
   "source": [
    "## Graph Contrastive Learning (GCL) — Pretraining Stage\n",
    "\n",
    "Ý tưởng chính:\n",
    "- Thay vì chỉ dựa vào nhãn, ta huấn luyện encoder học embedding bằng cách phân biệt giữa cặp **positive** (2 augmentation của cùng node/graph) và **negative** (augmentation của node khác).\n",
    "- Các augmentation phổ biến:\n",
    "  - Node drop (ngẫu nhiên bỏ một số node).\n",
    "  - Edge drop (ngẫu nhiên bỏ cạnh).\n",
    "  - Feature masking (che một số thuộc tính).\n",
    "- Loss: InfoNCE = maximize similarity của cặp positive, minimize similarity với negative.\n",
    "\n",
    "Pipeline GCL: Graph → Augment (view1, view2) → Encoder → Projection head → Contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba53bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_edges(data, drop_prob=0.2):\n",
    "    \"\"\"Randomly drop edges with probability drop_prob.\"\"\"\n",
    "    edge_index = data.edge_index\n",
    "    mask = torch.rand(edge_index.size(1)) > drop_prob\n",
    "    edge_index = edge_index[:, mask]\n",
    "    return Data(x=data.x, edge_index=edge_index, y=data.y)\n",
    "\n",
    "\n",
    "def mask_features(data, mask_prob=0.2):\n",
    "    \"\"\"Randomly mask node features.\"\"\"\n",
    "    x = data.x.clone()\n",
    "    mask = torch.rand_like(x) < mask_prob\n",
    "    x[mask] = 0\n",
    "    return Data(x=x, edge_index=data.edge_index, y=data.y)\n",
    "\n",
    "\n",
    "def graph_augment(data):\n",
    "    \"\"\"Apply a sequence of augmentations (can be randomized).\"\"\"\n",
    "    data_aug = drop_edges(data, drop_prob=0.2)\n",
    "    data_aug = mask_features(data_aug, mask_prob=0.2)\n",
    "    return data_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9cc7f",
   "metadata": {},
   "source": [
    "## 4. Model: Supervised baseline (GraphSAGE)\n",
    "\n",
    "Architecture:\n",
    "- Encoder: 2-layer GraphSAGE\n",
    "- Classifier: 2-layer MLP (embedding → hidden → logits)\n",
    "- Loss: CrossEntropy (or BCE if binary)\n",
    "- Training knobs: lr=1e-3, optimizer=Adam, epochs=..., weight decay, class weights if imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcef20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder = GraphSAGE(\n",
    "    in_dim=train_data.num_node_features,\n",
    "    hidden_dim=128,\n",
    "    out_dim=128\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68cae458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(torch.nn.Module):\n",
    "#     def __init__(self, in_dim, hidden_dim, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.net = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(in_dim, hidden_dim),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Dropout(0.5),\n",
    "#             torch.nn.Linear(hidden_dim, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "\n",
    "# classifier = MLP(in_dim=128, hidden_dim=128, num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d286d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, out_dim)\n",
    "        self.fc2 = torch.nn.Linear(out_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "\n",
    "projection = ProjectionHead(in_dim=128, out_dim=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465c5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_nce_loss(z1, z2, temperature=0.5):\n",
    "    # normalize\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    N = z1.size(0)\n",
    "    representations = torch.cat([z1, z2], dim=0)  # 2N x d\n",
    "    sim_matrix = torch.mm(representations, representations.t())  # 2N x 2N\n",
    "\n",
    "    # mask out self-contrast\n",
    "    mask = torch.eye(2*N, dtype=torch.bool, device=z1.device)\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, -9e15)\n",
    "\n",
    "    # positives: i-th sample in z1 with i-th in z2\n",
    "    positives = torch.cat(\n",
    "        [torch.arange(N, 2*N), torch.arange(0, N)]).to(z1.device)\n",
    "    logits = sim_matrix\n",
    "    labels = positives\n",
    "\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd60c8",
   "metadata": {},
   "source": [
    "## Training & Evaluation\n",
    "\n",
    "- Train loop outputs per-epoch: loss, training time/epoch.\n",
    "- Evaluation metrics:\n",
    "  - Detection metrics: Precision, Recall, F1 (per-class), AUC.\n",
    "  - Operational metrics: average inference latency per sample (ms), throughput (flows/sec), GPU/CPU memory usage.\n",
    "\n",
    "**Logging recommendations (in code):**\n",
    "- print time per epoch,\n",
    "- measure `inference_time = timeit()` per batch and report mean ms/sample,\n",
    "- keep `torch.cuda.empty_cache()` after heavy operations to reduce fragmentation (when debugging).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b334fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nodes(data, num_samples=5000):\n",
    "    idx = torch.randperm(data.num_nodes)[:num_samples]\n",
    "    return Data(\n",
    "        x=data.x[idx],\n",
    "        edge_index=data.edge_index,  # có thể filter subgraph cho đúng\n",
    "        y=data.y[idx] if data.y is not None else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4398ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_contrastive(data, encoder, projection, epochs=10, lr=1e-3):\n",
    "    encoder.train()\n",
    "    projection.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(encoder.parameters()) + list(projection.parameters()), lr=lr\n",
    "    )\n",
    "    for epoch in range(epochs):\n",
    "        data1 = graph_augment(sample_nodes(data, num_samples=5000))\n",
    "        data2 = graph_augment(sample_nodes(data, num_samples=5000))\n",
    "        z1 = encoder(data1.x.to(device), data1.edge_index.to(device))\n",
    "        z2 = encoder(data2.x.to(device), data2.edge_index.to(device))\n",
    "        z1 = projection(z1)\n",
    "        z2 = projection(z2)\n",
    "        loss = info_nce_loss(z1, z2)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"[Pretrain] Epoch {epoch}, Loss={loss.item():.4f}\")\n",
    "        \n",
    "\n",
    "print(\"=== Pretraining with GCL ===\")\n",
    "pretrain_contrastive(train_data, encoder, projection, epochs=20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e215a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def train_finetune(data, encoder, epochs=10, lr=1e-3):\n",
    "    encoder.eval()\n",
    "    classifier = Classifier(\n",
    "        in_dim=encoder.hidden_channels, num_classes=2).to(device)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        with torch.no_grad():\n",
    "            z = encoder(data.x.to(device), data.edge_index.to(device))\n",
    "        logits = classifier(z)\n",
    "        loss = F.cross_entropy(logits, data.y.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"[Finetune] Epoch {epoch}, Loss={loss.item():.4f}\")\n",
    "    return classifier\n",
    "\n",
    "print(\"=== Fine-tuning Classifier ===\")\n",
    "classifier = train_finetune(train_data, encoder, epochs=20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3\n",
    ")\n",
    "\n",
    "def evaluate(data):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        z = encoder(data.x.to(device), data.edge_index.to(device))\n",
    "        logits = classifier(z)\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=1)\n",
    "        y_true = data.y.cpu().numpy()\n",
    "\n",
    "    f1_w = f1_score(y_true, preds, average=\"weighted\")\n",
    "    f1_m = f1_score(y_true, preds, average=\"macro\")\n",
    "    auc = roc_auc_score(y_true, probs[:, 1])\n",
    "    print(f\"[Eval] F1_weighted={f1_w:.4f}, F1_macro={f1_m:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_true, preds))\n",
    "    return f1_w, f1_m, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Evaluation on Test ===\")\n",
    "evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8c172",
   "metadata": {},
   "source": [
    "## Explainability (XAI)\n",
    "\n",
    "We will provide multi-level explanations:\n",
    "1. **GNNExplainer (subgraph + feature mask)** — run per-alert (node). Output: compact subgraph (highlight edges/nodes) and feature importance mask for the target node.\n",
    "2. **Attention weights** (if using GAT/E-GraphSAGE) — visualize edge attention as thickness / opacity on edges.\n",
    "3. **Gradient-based heatmap** (Integrated Gradients / GraphGrad-CAM) — color nodes by importance (blue→red scale).\n",
    "4. **Feature importance (LIME / SHAP)** — per-flow attribute contribution (presented as small bar chart).\n",
    "\n",
    "**How to read the figures generated here:**\n",
    "- Subgraph highlight (GNNExplainer): red edges/nodes are most important; node labels show `(IP, port, count)` where available.\n",
    "- Gradient heatmap: node colors map to importance score (blue low → red high).\n",
    "- LIME/SHAP bar chart: top-k contributing features for the predicted label.\n",
    "\n",
    "**Evaluation of explanations (we report):** fidelity, sparsity, and a small human-evaluation checklist (10 alerts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e94989",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa full model để nối encoder + classifier\n",
    "class FullModel(torch.nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encoder(x, edge_index)   # embedding\n",
    "        out = self.classifier(z)          # logits\n",
    "        return F.log_softmax(out, dim=1)  # return_type='log_probs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "\n",
    "full_model_cpu = FullModel(encoder, classifier).to(device_cpu)\n",
    "test_data_cpu = test_data.to(device_cpu)\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=full_model_cpu,\n",
    "    algorithm=GNNExplainer(epochs=100),  # giảm epochs để tiết kiệm\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "\n",
    "node_id = 0\n",
    "explanation = explainer(\n",
    "    x=test_data_cpu.x, edge_index=test_data_cpu.edge_index, index=node_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature mask shape:\", explanation.node_mask.shape)\n",
    "print(\"Edge mask shape:\", explanation.edge_mask.shape)\n",
    "\n",
    "# Trực quan subgraph\n",
    "explanation.visualize_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
